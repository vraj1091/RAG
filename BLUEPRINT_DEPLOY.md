# üöÄ Complete Blueprint Deployment Guide

## Single Service Deployment (Backend + Frontend Together)

This guide deploys your entire RAG Chatbot as a single service using Docker, with:
- ‚úÖ Local SQLite database (no external DB needed)
- ‚úÖ Local AI model configuration (Ollama)
- ‚úÖ Frontend and Backend served together
- ‚úÖ One-click Blueprint deployment

---

## üìã What This Deploys

**Single Docker Container includes:**
- FastAPI Backend (Python)
- React Frontend (built and served as static files)
- SQLite Database (local file)
- Vector Store (ChromaDB - local)
- File uploads (local storage)

**Configuration:**
- Database: SQLite (automatic, no setup)
- AI Model: llama3.2:latest (local Ollama)
- Storage: Local filesystem
- Port: 8000 (serves both API and frontend)

---

## üöÄ Deploy to Render with Blueprint

### Step 1: Push Code to GitHub

Your code is already at: https://github.com/vraj1091/RAG

Make sure latest changes are pushed:
```bash
cd ai-rag-chatbot-python_v7
git add .
git commit -m "Add Blueprint deployment configuration"
git push origin main
```

### Step 2: Deploy on Render

1. **Go to Render Dashboard**: https://dashboard.render.com

2. **Create New Blueprint**:
   - Click **"New +"** ‚Üí **"Blueprint"**
   - Connect your GitHub account if needed
   - Select repository: **`vraj1091/RAG`**
   - Render will detect `render.yaml` automatically

3. **Review Configuration**:
   - Service Name: `rag-chatbot`
   - Type: Web Service (Docker)
   - Region: Oregon (Free)
   - Branch: main

4. **Environment Variables** (auto-configured from render.yaml):
   ```
   DB_HOST=localhost (SQLite fallback)
   SECRET_KEY=<auto-generated>
   OLLAMA_MODEL_NAME=llama3.2:latest
   DEBUG=false
   ```

5. **Click "Apply"**:
   - Render will build the Docker image
   - This takes 10-15 minutes (first time)
   - Watch the build logs

### Step 3: Wait for Deployment

**Build Process:**
1. ‚úÖ Building frontend (npm install + build) - 3-5 min
2. ‚úÖ Building backend (pip install) - 5-7 min
3. ‚úÖ Creating Docker image - 2-3 min
4. ‚úÖ Deploying service - 1-2 min

**Total Time: ~15 minutes**

### Step 4: Access Your App

Once deployed, you'll get a URL like:
```
https://rag-chatbot-XXXX.onrender.com
```

**Test URLs:**
- Frontend: `https://rag-chatbot-XXXX.onrender.com/`
- API Docs: `https://rag-chatbot-XXXX.onrender.com/docs`
- Health: `https://rag-chatbot-XXXX.onrender.com/health`

---

## ‚úÖ What Works Out of the Box

### ‚úÖ Working Features:
- User Registration & Login
- Document Upload (PDF, DOCX, TXT, Images)
- Knowledge Base Management
- Chat Interface
- SQLite Database (automatic)
- File Storage (ephemeral on free tier)
- Vector Store (ChromaDB)

### ‚ö†Ô∏è Limited on Free Tier:
- **AI Responses**: Ollama won't work (needs GPU)
  - Chat interface works
  - But AI won't generate responses
  - **Solution**: Use external AI API (see below)

- **File Persistence**: Files reset on restart
  - Normal for Render free tier
  - **Solution**: Add external storage or upgrade

- **Service Sleep**: Sleeps after 15 min inactivity
  - First request takes 30-60s to wake
  - **Solution**: Upgrade to paid ($7/month)

---

## üîß Fix AI Model Issue (Recommended)

Since Ollama won't work on Render free tier, use an external AI API:

### Option 1: Use OpenAI (Best)

1. Get API key from: https://platform.openai.com/api-keys

2. Update Environment Variables in Render:
   - Go to your service ‚Üí Environment
   - Add:
     ```
     OPENAI_API_KEY=sk-your-key-here
     USE_OPENAI=true
     ```
   - Save and redeploy

3. Update backend code to use OpenAI (if not already configured)

### Option 2: Use Google Gemini (Free)

1. Get API key from: https://makersuite.google.com/app/apikey

2. Add to Render Environment:
   ```
   GEMINI_API_KEY=your-key-here
   USE_GEMINI=true
   ```

### Option 3: Deploy on Railway Instead

Railway has better free tier with GPU support:
- Go to https://railway.app
- Deploy from GitHub
- Ollama will work there!

---

## üìä Monitor Your Deployment

### View Logs

1. Go to your service in Render
2. Click **"Logs"** tab
3. Monitor real-time logs

**Expected Log Messages:**
```
‚úÖ Application startup complete!
‚úÖ Database tables initialized
‚úÖ Frontend static files mounted
‚ö†Ô∏è Ollama connection failed (expected on free tier)
```

### Check Health

Visit: `https://your-app.onrender.com/health`

Should return:
```json
{
  "status": "healthy",
  "components": {
    "database": {"status": "healthy"},
    "vector_db": {"status": "healthy"},
    "deepseek": {"status": "available"}
  }
}
```

---

## üêõ Troubleshooting

### Build Fails

**Check build logs for:**
- Missing dependencies
- Node/Python version issues
- Out of memory errors

**Solutions:**
```bash
# Test locally first
docker build -t rag-chatbot .
docker run -p 8000:8000 rag-chatbot
```

### Service Won't Start

**Common Issues:**
1. Port not exposed correctly
2. Environment variables missing
3. Database connection failing

**Check:**
- Logs show "Application startup complete"
- Health endpoint returns 200
- No critical errors in logs

### Frontend Not Loading

**Check:**
1. Build completed successfully
2. Static files mounted correctly
3. Root endpoint serves index.html

**Test:**
```bash
curl https://your-app.onrender.com/
# Should return HTML, not JSON
```

### Can't Login/Register

**Verify:**
1. Database initialized (check logs)
2. Backend API accessible
3. CORS configured correctly

**Test API:**
```bash
curl https://your-app.onrender.com/docs
# Should show API documentation
```

---

## üîÑ Update Your Deployment

### Push Updates

```bash
git add .
git commit -m "Update feature"
git push origin main
```

Render will automatically:
1. Detect the push
2. Rebuild the Docker image
3. Deploy the new version
4. Takes ~10-15 minutes

### Manual Redeploy

1. Go to your service in Render
2. Click **"Manual Deploy"**
3. Select **"Deploy latest commit"**

---

## üí∞ Cost Breakdown

### Free Tier (Current):
- Web Service: Free (with sleep)
- Build Minutes: Unlimited
- Bandwidth: 100 GB/month
- **Total: $0/month**

### Paid Tier (Always On):
- Web Service: $7/month
- No sleep
- More resources
- **Total: $7/month**

---

## üéØ Production Checklist

Before going live:

- [ ] Test all features work
- [ ] Add external AI API (OpenAI/Gemini)
- [ ] Set up custom domain
- [ ] Configure SSL (automatic on Render)
- [ ] Add monitoring/alerts
- [ ] Set up backups
- [ ] Add external storage (S3) for files
- [ ] Add PostgreSQL for production DB
- [ ] Upgrade to paid plan (no sleep)

---

## üìù Environment Variables Reference

All configured in `render.yaml`:

```yaml
# Database (SQLite fallback)
DB_HOST=localhost
DB_PORT=3306
DB_NAME=rag_chatbot
DB_USER=root
DB_PASSWORD=<auto-generated>

# Security
SECRET_KEY=<auto-generated>
ACCESS_TOKEN_EXPIRE_MINUTES=30

# AI Model (Local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL_NAME=llama3.2:latest
MODEL_TEMPERATURE=0.7

# Application
DEBUG=false
LOG_LEVEL=INFO
PORT=8000

# Storage (Local)
UPLOAD_PATH=./uploads
VECTOR_STORE_PATH=./vector_store
MAX_FILE_SIZE_MB=50
```

---

## üÜò Need Help?

1. **Check Logs First**: Most issues show up in logs
2. **Test Locally**: Run Docker container locally
3. **Verify Build**: Ensure Docker build succeeds
4. **Check Status**: https://status.render.com

---

## ‚úÖ Success Indicators

Your deployment is successful when:

1. ‚úÖ Build completes without errors
2. ‚úÖ Service shows "Live" status
3. ‚úÖ Health check returns healthy
4. ‚úÖ Frontend loads at root URL
5. ‚úÖ API docs accessible at /docs
6. ‚úÖ Can register new user
7. ‚úÖ Can login successfully
8. ‚úÖ Chat interface loads

---

**üéâ Your RAG Chatbot is now deployed!**

**Single URL for everything:**
- Frontend: `https://rag-chatbot-XXXX.onrender.com/`
- API: `https://rag-chatbot-XXXX.onrender.com/api/v1/`
- Docs: `https://rag-chatbot-XXXX.onrender.com/docs`

**Next Steps:**
1. Add external AI API for chat responses
2. Test all features
3. Share with users!

---

**Deployment Time: ~15 minutes**
**Cost: $0/month (free tier)**
**Maintenance: Automatic updates on git push**
